
~~ Licensed under the Apache License, Version 2.0 (the "License");
~~ you may not use this file except in compliance with the License.
~~ You may obtain a copy of the License at
~~
~~   http://www.apache.org/licenses/LICENSE-2.0
~~
~~ Unless required by applicable law or agreed to in writing, software
~~ distributed under the License is distributed on an "AS IS" BASIS,
~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
~~ See the License for the specific language governing permissions and
~~ limitations under the License. See accompanying LICENSE file.

  ---
  Hadoop Distributed File System-${project.version} - Short-Circuit Local Reads
  ---
  ---
  ${maven.build.timestamp}

HDFS Short-Circuit Local Reads

%{toc|section=1|fromDepth=0}

* {Short-Circuit Local Reads}

** Background

  In <<<HDFS>>>, reads normally go through the <<<DataNode>>>.  Thus, when the
  client asks the <<<DataNode>>> to read a file, the <<<DataNode>>> reads that
  file off of the disk and sends the data to the client over a TCP socket.
  So-called "short-circuit" reads bypass the <<<DataNode>>>, allowing the client
  to read the file directly.  Obviously, this is only possible in cases where
  the client is co-located with the data.  Short-circuit reads provide a
  substantial performance boost to many applications.

** Setup

  To configure short-circuit local reads, you will need to enable
  <<<libhadoop.so>>>.  See
  {{{../hadoop-common/NativeLibraries.html}Native
  Libraries}} for details on enabling this library.

  Short-circuit reads make use of a UNIX domain socket.  This is a special path
  in the filesystem that allows the client and the DataNodes to communicate.
  You will need to set a path to this socket.  The DataNode needs to be able to
  create this path.  On the other hand, it should not be possible for any user
  except the HDFS user or root to create this path.  For this reason, paths
  under <<</var/run>>> or <<</var/lib>>> are often used.

  Client and DataNode uses shared memory segments
  to communicate short-circuit read.
  if /dev/shm is not world writable or does not exist in your environment,
  You can change the paths on which shared memory segments are created by
  setting the value of <<<dfs.datanode.shared.file.descriptor.paths>>>
  to comma separated paths like <<</dev/shm,/tmp>>>.
  It tries paths in order until creation succeeds.
  
  Short-circuit local reads need to be configured on both the <<<DataNode>>>
  and the client.

** Example Configuration

  Here is an example configuration.

----
<configuration>
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value>/var/lib/hadoop-hdfs/dn_socket</value>
  </property>
</configuration>
----

* Legacy HDFS Short-Circuit Local Reads

  Legacy short-circuit local reads implementation
  on which clients directly open HDFS block files is still available
  for platforms other than Linux.
  Setting the value of <<<dfs.client.use.legacy.blockreader.local>>>
  in addition to <<<dfs.client.read.shortcircuit>>>
  to true enables this feature.
  Only users listed in the value of
  <<<dfs.block.local-path-access.user>>> can use this feature.

----
<configuration>
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.client.use.legacy.blockreader.local</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.block.local-path-access.user</name>
    <value>foo,bar</value>
  </property>
</configuration>
----

* {Zero-Copy Short-Circuit Local Reads}

  <<<HdfsDataInputStream>>> (and <<<DFSInputStream>>>) provides
  so-called zero-copy feature which is the fast short-circuit local reads
  avoiding unnecessary data copy in OS kernel by using mmap.
  Though (legacy) {{Short-Circuit Local Reads}} is application transparent,
  {{Zero-Copy Short-Circuit Local Reads}} requires its own client API
  shown below.

----
  public ByteBuffer read(ByteBufferPool factory, int maxLength,
      EnumSet<ReadOption> opts)
          throws IOException, UnsupportedOperationException;

  public void releaseBuffer(ByteBuffer buffer);
----

  <<<read>>> method returns direct ByteBuffer containing file data.

  * By passing
    {{{../../api/org/apache/hadoop/io/ByteBufferPool.html}<<<ByteBufferPool>>>}}
    implementation as factory argument,
    it fallbacks to normal read path when zero-copy is not available and
    ByteBuffer got from pool containing read data.

  * If factory argument is null,
    <<<UnsupportedOperationException>>> will be thrown
    when zero-copy is not available.

  * Zero-copy read is used only in skipping checksum mode,
    i.e. opts contains <<<ReadOption.SKIP_CHECKSUMS>>> or
    <<<dfs.client.read.shortcircuit.skip.checksum>>> is set to true.

  []
  
  Client must close used ByteBuffer by calling <<<releaseBuffer>>> method.

  See also 
  {{{../../api/org/apache/hadoop/fs/FSDataInputStream.html}
  Javadoc of
  <<<read(ByteBufferPool bufferPool, int maxLength, EnumSet<ReadOption> opts)>>>
  method and <<<releaseBuffer(ByteBuffer buffer)>>> method}}.



* Configuration Properties

  Information used by short-circuit read is cached in client
  beyond life spans of each InputStreams.
  There are some configuration parameters to control the size of
  this client cache.

*-------------------------+------------------------+
|| Key                    || Notes                 |
*-------------------------+------------------------+
| <<<dfs.client.read.shortcircuit>>> | |
| | This configuration parameter turns on short-circuit local reads. |
*-------------------------+-------------------------+------------------------+
| <<<dfs.domain.socket.path>>> | |
| | The file path of UNIX domain socket used by short-circuit local read. |
*-------------------------+-------------------------+------------------------+
| <<<dfs.client.read.shortcircuit.skip.checksum>>> | |
| | If this configuration parameter is set, |
| | short-circuit local reads will skip checksums. |
| | This is normally not recommended, |
| | but it may be useful for special setups. |
| | You might consider using this |
| | if you are doing your own checksumming outside of HDFS. |
*-------------------------+-------------------------+------------------------+
| <<<dfs.client.read.shortcircuit.streams.cache.size>>> | |
| | The DFSClient maintains a cache of recently opened file descriptors. |
| | This parameter controls the size of that cache. |
| | Setting this higher will use more file descriptors, |
| | but potentially provide better performance on workloads |
| | involving lots of seeks. |
*-------------------------+-------------------------+------------------------+
| <<<dfs.client.read.shortcircuit.streams.cache.expiry.ms>>> | |
| | This controls the minimum amount of time |
| | file descriptors need to sit in the client cache context |
| | before they can be closed for being inactive for too long. |
*-------------------------+-------------------------+------------------------+
| <<<dfs.client.use.legacy.blockreader.local>>> | |
| | Legacy short-circuit reader implementation based on HDFS-2246 is used |
| | if this configuration parameter is true. |
| | This is for platforms where the new implementation based on HDFS-347 |
| | is not available. |
*-------------------------+-------------------------+------------------------+
| <<<dfs.block.local-path-access.user>>> | |
| | Comma separated list of users allowd to open block files |
| | on legacy short-circuit local read. |
*-------------------------+-------------------------+------------------------+
| <<<dfs.client.domain.socket.data.traffic>>> | |
| | This control whether we will try to pass normal data traffic |
| | over UNIX domain socket rather than over TCP socket |
| | on node-local data transfer. |
| | This is experimental feature now and is turned off by default. |
*-------------------------+-------------------------+------------------------+
